{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies low pass filtering\n",
    "def lpFilter(exp):\n",
    "    sos = signal.butter(2,5,btype='lowpass',output='sos', analog=False, fs=1/0.01)\n",
    "    features = ['ax','ay','az','mx','my','mz','gx','gy','gz']\n",
    "\n",
    "    for i in features:\n",
    "        exp[i] = signal.sosfilt(sos,exp[i])\n",
    "\n",
    "def getZeroCrossingRate(arr):\n",
    "    my_array = np.array(arr)\n",
    "    return float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) < 0).sum())/len(arr)))\n",
    "def getMeanCrossingRate(arr):\n",
    "    return getZeroCrossingRate(np.array(arr) - np.mean(arr))\n",
    "\n",
    "\n",
    "# raw experiment data, combine imu and video data and create time window splits\n",
    "def augmentation(exp,expXY):\n",
    "    exp.yaw = exp.yaw - exp.iloc[0].yaw\n",
    "    exp['t'] = exp.index * expXY.t.max()/len(exp)\n",
    "\n",
    "    index=[0]\n",
    "    epsilon = 0.01\n",
    "    for i in expXY.index:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        index.append(index[-1])\n",
    "        while index[-1] < len(exp.index)-1 and abs(exp.iloc[index[-1]].t - expXY.iloc[i].t) > epsilon:\n",
    "            index[-1] += 1\n",
    "\n",
    "    newexp = pd.concat([exp.iloc[index].drop(columns=['t']).reset_index(drop=True), expXY], join='outer', axis=1)\n",
    "    newexp.head()\n",
    "\n",
    "    # get intervals\n",
    "    # 1 sec intervals w/ 0.25 sec overlap\n",
    "    rowdeltaT = newexp.t.max()/len(newexp.index)\n",
    "    numRows = (round)(1/rowdeltaT)\n",
    "    numRowsOverlap = (round)(0.25/rowdeltaT)\n",
    "\n",
    "    indexPairs = []\n",
    "    currentIndex = 0 \n",
    "    while currentIndex+numRows < len(newexp.index):\n",
    "        indexPairs.append((currentIndex, currentIndex+numRows))\n",
    "        currentIndex += numRows-numRowsOverlap\n",
    "\n",
    "    stats = ['avg_', 'std_', 'kurt_']#,'var_','rms_','med_','zcr_','mcr_']\n",
    "    features = ['ax','ay','az','mx','my','mz','gx','gy','gz','yaw','pitch','roll']\n",
    "    finalexp = {}\n",
    "    for i in features:\n",
    "        for e in stats:\n",
    "            finalexp[e+i] = []\n",
    "    finalexp['dx'] = []\n",
    "    finalexp['dy'] = []\n",
    "\n",
    "    for i in indexPairs:\n",
    "        data = newexp.iloc[i[0]:i[1]]\n",
    "        for i in features:\n",
    "            finalexp[stats[0]+i].append(data[i].mean())\n",
    "            finalexp[stats[1]+i].append(data[i].std())\n",
    "            finalexp[stats[2]+i].append(data[i].kurt())\n",
    "            # finalexp[stats[3]+i].append(data[i].var())\n",
    "            # finalexp[stats[4]+i].append(np.sqrt(data[i].mean()*data[i].mean()))\n",
    "            # finalexp[stats[5]+i].append(data[i].median())\n",
    "            # finalexp[stats[6]+i].append(getZeroCrossingRate(data[i]))\n",
    "            # finalexp[stats[7]+i].append(getMeanCrossingRate(data[i]))\n",
    "        finalexp['dx'].append(data.iloc[-1].x - data.iloc[0].x)\n",
    "        finalexp['dy'].append(data.iloc[-1].y - data.iloc[0].y)\n",
    "\n",
    "    return pd.DataFrame(finalexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_az     0.226646\n",
       "avg_ay     0.121036\n",
       "std_mx     0.096383\n",
       "avg_gy     0.076980\n",
       "std_gx     0.074002\n",
       "std_my     0.067339\n",
       "std_yaw    0.065085\n",
       "std_gz     0.063159\n",
       "avg_yaw    0.042978\n",
       "std_mz     0.036426\n",
       "Name: dx, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads all datasets and stores in exp array\n",
    "exp = {}\n",
    "numExp = 9\n",
    "for i in range(9):\n",
    "    expTemp = pd.read_csv('data/exp' + str(i+1) + '.csv')\n",
    "    expXYTemp = pd.read_csv('data/exp' + str(i+1) + 'XY.csv')\n",
    "    lpFilter(expTemp)\n",
    "    exp[i+1] = augmentation(expTemp,expXYTemp)\n",
    "\n",
    "# creates finalexp\n",
    "include = [exp[1],exp[2],exp[3],exp[4],exp[6],exp[7],exp[8],exp[9]]\n",
    "finalexp = pd.concat(include,join='inner')\n",
    "finalexp.reset_index(drop=True,inplace=True)\n",
    "\n",
    "finalexp.tail()\n",
    "finalexp.drop(columns=['dy']).corr().dx.sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data\n",
    "features = finalexp.drop(columns=['dy']).corr().dx.sort_values(ascending=False).index[1:11]\n",
    "X = finalexp[features]\n",
    "Y = finalexp.dx\n",
    "\n",
    "num_splits = 10\n",
    "kf = KFold(n_splits=num_splits, shuffle=True,random_state=42)\n",
    "stdsclr = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'min_samples_leaf': 15, 'min_samples_split': 2}\n",
      "0.6875414103981742\n"
     ]
    }
   ],
   "source": [
    "# decision tree regression\n",
    "dregr = DecisionTreeRegressor(random_state=42)\n",
    "dt_params = {'max_features': [None, 1.0,5,'sqrt','log2'], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "d_clf = GridSearchCV(estimator=dregr,param_grid=dt_params,cv=num_splits)\n",
    "\n",
    "d_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(d_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "dregr = DecisionTreeRegressor(max_features=d_clf.best_params_['max_features'], min_samples_split=d_clf.best_params_['min_samples_split'],min_samples_leaf=d_clf.best_params_['min_samples_leaf'], random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    dregr.fit(trainX, trainY)\n",
    "    score += dregr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.7504549554817971\n"
     ]
    }
   ],
   "source": [
    "# random forests regression\n",
    "regr = RandomForestRegressor()\n",
    "rf_params = {'max_features': [None, 1.0,5,'sqrt','log2'],'n_estimators': [5,10,20,30,50,100], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "r_clf = GridSearchCV(estimator=regr,param_grid=rf_params,cv=num_splits)\n",
    "\n",
    "r_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(r_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "regr = RandomForestRegressor(max_features=r_clf.best_params_['max_features'],min_samples_leaf=r_clf.best_params_['min_samples_leaf'], min_samples_split=r_clf.best_params_['min_samples_split'],n_estimators=r_clf.best_params_['n_estimators'],random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    regr.fit(trainX, trainY)\n",
    "    score += regr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'min_samples_leaf': 7, 'min_samples_split': 15, 'n_estimators': 10}\n",
      "0.5855171469102359\n"
     ]
    }
   ],
   "source": [
    "# gradient boosted tree regression\n",
    "gregr = GradientBoostingRegressor()\n",
    "gb_params = {'max_features': [None, 1.0,5,'sqrt','log2'],'n_estimators': [5,10,20,30,50,100], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "g_clf = GridSearchCV(estimator=gregr,param_grid=gb_params,cv=num_splits)\n",
    "\n",
    "g_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(g_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "gregr = GradientBoostingRegressor(max_features=g_clf.best_params_['max_features'],min_samples_leaf=g_clf.best_params_['min_samples_leaf'], min_samples_split=g_clf.best_params_['min_samples_split'],n_estimators=g_clf.best_params_['n_estimators'],random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    gregr.fit(trainX, trainY)\n",
    "    score += gregr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "MinMax:0.6875414103981742\n",
      "Norm:0.5389971241715183\n",
      "Robust:0.6875414103981742\n",
      "\n",
      "Random Forests Model\n",
      "MinMax:0.7504413791309332\n",
      "Norm:0.6959241003439114\n",
      "Robust:0.7504561244674429\n",
      "\n",
      "Gradient Boosting Model\n",
      "MinMax:0.5855171469102359\n",
      "Norm:0.49603133338614463\n",
      "Robust:0.5855171469102359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try different preprocessing\n",
    "minmaxsclr = MinMaxScaler()\n",
    "normsclr = Normalizer()\n",
    "rbsclr = RobustScaler(quantile_range=(25, 75))\n",
    "\n",
    "sclrs = [minmaxsclr,normsclr,rbsclr]\n",
    "\n",
    "# on decision trees\n",
    "scores = {'Decision Tree': [0,0,0], 'Random Forests': [0,0,0], 'Gradient Boosting': [0,0,0]}\n",
    "for i in range(3):\n",
    "    for (train, test) in kf.split(X):\n",
    "        trainX = sclrs[i].fit_transform(X.iloc[train])\n",
    "        trainY = Y.iloc[train]\n",
    "        testX = sclrs[i].transform(X.iloc[test])\n",
    "        testY = Y.iloc[test]\n",
    "        dregr.fit(trainX, trainY)\n",
    "        scores['Decision Tree'][i] += dregr.score(testX,testY)\n",
    "        regr.fit(trainX, trainY)\n",
    "        scores['Random Forests'][i] += regr.score(testX,testY)\n",
    "        gregr.fit(trainX, trainY)\n",
    "        scores['Gradient Boosting'][i] += gregr.score(testX,testY)\n",
    "    scores['Decision Tree'][i] /= num_splits\n",
    "    scores['Random Forests'][i] /= num_splits\n",
    "    scores['Gradient Boosting'][i] /= num_splits\n",
    "\n",
    "for i in scores.keys():\n",
    "    print(i+\" Model\")\n",
    "    print('MinMax:' + str(scores[i][0]))\n",
    "    print('Norm:' + str(scores[i][1]))\n",
    "    print('Robust:' + str(scores[i][2]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_ay       0.225051\n",
      "std_mx       0.189047\n",
      "std_yaw      0.072882\n",
      "avg_az       0.068624\n",
      "kurt_mx      0.053175\n",
      "avg_yaw      0.050325\n",
      "kurt_az      0.050188\n",
      "kurt_ay      0.034056\n",
      "kurt_ax      0.026149\n",
      "avg_pitch    0.020075\n",
      "Name: dy, dtype: float64\n",
      "\n",
      "DecisionTree\n",
      "{'max_features': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.6176472834637319\n",
      "\n",
      "Random Forests\n",
      "{'max_features': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.7293022334543902\n",
      "\n",
      "Gradient Boosting\n",
      "{'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 20}\n",
      "0.6275386667750651\n",
      "\n",
      "Preprocessing effect\n",
      "Decision Tree Model\n",
      "MinMax:0.6176472834637319\n",
      "Norm:0.4698743361304413\n",
      "Robust:0.617656191301047\n",
      "\n",
      "Random Forests Model\n",
      "MinMax:0.7290548151268974\n",
      "Norm:0.6325581090088523\n",
      "Robust:0.7291967292699522\n",
      "\n",
      "Gradient Boosting Model\n",
      "MinMax:0.6275386667750651\n",
      "Norm:0.5467116595030923\n",
      "Robust:0.6276933124068129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repeat for dy\n",
    "# prep data\n",
    "print(finalexp.drop(columns=['dx']).corr().dy.sort_values(ascending=False)[1:11])\n",
    "print()\n",
    "features = finalexp.drop(columns=['dx']).corr().dy.sort_values(ascending=False).index[1:11]\n",
    "X = finalexp[features]\n",
    "Y = finalexp.dy\n",
    "\n",
    "num_splits = 10\n",
    "kf = KFold(n_splits=num_splits, shuffle=True,random_state=42)\n",
    "stdsclr = StandardScaler()\n",
    "\n",
    "# decision tree regression\n",
    "print('DecisionTree')\n",
    "dregr = DecisionTreeRegressor(random_state=42)\n",
    "dt_params = {'max_features': [None, 1.0,5,'sqrt','log2'], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "d_clf = GridSearchCV(estimator=dregr,param_grid=dt_params,cv=num_splits)\n",
    "\n",
    "d_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(d_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "dregr = DecisionTreeRegressor(max_features=d_clf.best_params_['max_features'], min_samples_split=d_clf.best_params_['min_samples_split'],min_samples_leaf=d_clf.best_params_['min_samples_leaf'], random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    dregr.fit(trainX, trainY)\n",
    "    score += dregr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)\n",
    "print()\n",
    "\n",
    "# random forests regression\n",
    "print('Random Forests')\n",
    "regr = RandomForestRegressor()\n",
    "rf_params = {'max_features': [None, 1.0,5,'sqrt','log2'],'n_estimators': [5,10,20,30,50,100], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "r_clf = GridSearchCV(estimator=regr,param_grid=rf_params,cv=num_splits)\n",
    "\n",
    "r_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(r_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "regr = RandomForestRegressor(max_features=r_clf.best_params_['max_features'],min_samples_leaf=r_clf.best_params_['min_samples_leaf'], min_samples_split=r_clf.best_params_['min_samples_split'],n_estimators=r_clf.best_params_['n_estimators'],random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    regr.fit(trainX, trainY)\n",
    "    score += regr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)\n",
    "print()\n",
    "\n",
    "# gradient boosted tree regression\n",
    "print('Gradient Boosting')\n",
    "gregr = GradientBoostingRegressor()\n",
    "gb_params = {'max_features': [None, 1.0,5,'sqrt','log2'],'n_estimators': [5,10,20,30,50,100], 'min_samples_split': [2,5,7,10,15], 'min_samples_leaf': [1,5,7,10,15,20]}\n",
    "g_clf = GridSearchCV(estimator=gregr,param_grid=gb_params,cv=num_splits)\n",
    "\n",
    "g_clf.fit(stdsclr.fit_transform(X),Y)\n",
    "print(g_clf.best_params_)\n",
    "\n",
    "# kfold score\n",
    "score = 0\n",
    "gregr = GradientBoostingRegressor(max_features=g_clf.best_params_['max_features'],min_samples_leaf=g_clf.best_params_['min_samples_leaf'], min_samples_split=g_clf.best_params_['min_samples_split'],n_estimators=g_clf.best_params_['n_estimators'],random_state=42)\n",
    "for (train, test) in kf.split(X):\n",
    "    trainX = stdsclr.fit_transform(X.iloc[train])\n",
    "    trainY = Y.iloc[train]\n",
    "    testX = stdsclr.transform(X.iloc[test])\n",
    "    testY = Y.iloc[test]\n",
    "    gregr.fit(trainX, trainY)\n",
    "    score += gregr.score(testX,testY)\n",
    "\n",
    "score /= num_splits\n",
    "print(score)\n",
    "print()\n",
    "\n",
    "# try different preprocessing\n",
    "print('Preprocessing effect')\n",
    "minmaxsclr = MinMaxScaler()\n",
    "normsclr = Normalizer()\n",
    "rbsclr = RobustScaler(quantile_range=(25, 75))\n",
    "\n",
    "sclrs = [minmaxsclr,normsclr,rbsclr]\n",
    "\n",
    "# on decision trees\n",
    "scores = {'Decision Tree': [0,0,0], 'Random Forests': [0,0,0], 'Gradient Boosting': [0,0,0]}\n",
    "for i in range(3):\n",
    "    for (train, test) in kf.split(X):\n",
    "        trainX = sclrs[i].fit_transform(X.iloc[train])\n",
    "        trainY = Y.iloc[train]\n",
    "        testX = sclrs[i].transform(X.iloc[test])\n",
    "        testY = Y.iloc[test]\n",
    "        dregr.fit(trainX, trainY)\n",
    "        scores['Decision Tree'][i] += dregr.score(testX,testY)\n",
    "        regr.fit(trainX, trainY)\n",
    "        scores['Random Forests'][i] += regr.score(testX,testY)\n",
    "        gregr.fit(trainX, trainY)\n",
    "        scores['Gradient Boosting'][i] += gregr.score(testX,testY)\n",
    "    scores['Decision Tree'][i] /= num_splits\n",
    "    scores['Random Forests'][i] /= num_splits\n",
    "    scores['Gradient Boosting'][i] /= num_splits\n",
    "\n",
    "for i in scores.keys():\n",
    "    print(i+\" Model\")\n",
    "    print('MinMax:' + str(scores[i][0]))\n",
    "    print('Norm:' + str(scores[i][1]))\n",
    "    print('Robust:' + str(scores[i][2]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
